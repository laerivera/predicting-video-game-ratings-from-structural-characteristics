{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1267cefd-f1f4-46ec-b484-447baaabae11",
   "metadata": {},
   "source": [
    "# G.G, Good Game! Investigating the Structural Characteristics that Make Highly-Rated Video Games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb8630-7364-42e8-bd56-494fb4d6fd56",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook 4 - Modeling\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**By: [Lae Rivera](https://github.com/laerivera)**\n",
    "\n",
    "**BrainStation Data Science**\n",
    "\n",
    "**April 4, 2022**\n",
    "\n",
    "In this notebook, I will continue my preparation for the implementation of Natural Language Processing (NLP) and Machine Learning techniques. The processes outlined in this notebook includes: separating the documents into separate 'tokens' through a custom tokenizer, defining variables, establishing performance metrics, and finally, fitting my models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f8cea-49a1-416f-acc5-76488adfc3a0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b01af5-f29a-460b-a6a5-9cfc2de497a6",
   "metadata": {},
   "source": [
    "## Notebook Contents\n",
    "\n",
    "1. [**Create Custom Tokenizer**](#d1)<br>\n",
    "    1.1 [Implement Stemmer](#d1.1)<br>\n",
    "    1.2 [Remove Punctuation](#d1.2)<br>\n",
    "    1.3 [Remove Stopwords](#d1.3)<br>\n",
    "    1.4 [Final Tokenizer](#d1.4)<br>\n",
    "    \n",
    "2. [**Define Dependent and Independent Variables**](#d2)<br>\n",
    "    2.1 [Train/Test Split](#d2.1)<br>\n",
    "    2.2 [Distribution of Target Variable](#d2.2)<br>\n",
    "    \n",
    "3. [**Vectorizers Used**](#d3)\n",
    "\n",
    "4. [**Establish Performance Baseline**](#d4)<br>\n",
    "    3.1 [Dummy Classifier (TF-IDF)](#d4.1)<br>\n",
    "    3.2 [Dummy Results](#d4.2)<br>\n",
    "    \n",
    "5. [**Helper Function: Model File Loader**](#d5)<br>\n",
    "\n",
    "6. [**Modeling**](#d6)<br>\n",
    "\n",
    "7. [**Grid Search 1: Control**](#d7)<br>\n",
    "    6.1 [GridSearch 1 Results](#d7.1)<br>\n",
    "    6.2 [GridSearch 1 Summary](#d7.2)<br>\n",
    "    \n",
    "8. [**Grid Search 2: Ngrams**](#d8)<br>\n",
    "    7.1 [GridSearch 2 Results](#d8.1)<br>\n",
    "    7.2 [GridSearch 2 Summary](#d8.2)<br>\n",
    "    \n",
    "9. [**Grid Search 3: Feature Engineering**](#d9)<br>\n",
    "    8.1 [Creating Features](#d9.1)<br>\n",
    "    8.2 [GridSearch 3 Results](#d9.2)<br>\n",
    "    8.3 [GridSearch 3 Summary](#d9.3)<br>\n",
    "    \n",
    "10. [**Grid Search 4: Feature Engineering - Wider Range**](#d10)<br>\n",
    "    9.1 [GridSearch 4 Results](#d10.1)<br>\n",
    "    9.2 [GridSearch 4 Summary](#d10.2)<br>\n",
    "    \n",
    "11. [**Model Summary**](#d11)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f5916-4d1c-4d2c-944f-e3d1000cdd19",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba5f77-c6cf-4d21-9978-b194e2829d34",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eba767f-4264-45b9-86a6-c96a5162d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "# Main Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "\n",
    "# NLP \n",
    "import nltk\n",
    "import string\n",
    "import scipy.sparse\n",
    "\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Dummy \n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Models\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6171419-7778-4db7-86f0-afc283de4add",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f99f315-c582-456a-93f6-67a7239b9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "games = joblib.load('data/games.pkl')\n",
    "games_clean = joblib.load('data/games_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb1d2f-ee6a-41a4-9438-ab0bfef4acf2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a089b4-4833-445d-a2fb-124fd848efa5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Custom Tokenizer<a id=\"d1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a291cd-5006-422b-96a7-7e03e4ae7092",
   "metadata": {},
   "source": [
    "To get my model to understand the text data I would need to tokenize it, but first, here are a list of definitions that will be used moving forward:\n",
    "\n",
    "\n",
    "| Term  | Definition| |\n",
    "| -------| -------- | --- |\n",
    "|**Corpus** |           | The entire collection of individual text files in the whole dataset. | |\n",
    "|**Document** |           | A single or individual text file. | |\n",
    "|**Token** |           | Processed words from the document; can be multiple words. | |\n",
    "|**Vocabulary** |           | Collection of the tokens from the corpus. | |\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "This process essentially breaks down the individual documents into a sequence of tokens (individual/paired words depending on settings). What I am performing here is **feature extraction**——by obtaining certain features from a corpus that models will do analysis on.\n",
    "\n",
    "**Workflow**:\n",
    "- Implement Stemmer\n",
    "- Remove Punctuation\n",
    "- Remove Stopwords\n",
    "- These steps will go into a custom tokenizer so the steps are all together. This makes things more efficient and simple. \n",
    "\n",
    "**Sources**:\n",
    "- [nltk](https://www.nltk.org/)\n",
    "- [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "-[TF-IDF Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4b1ea-1b0e-45a4-b7c4-89f3a7bdd725",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Stemmer <a id=\"d1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47640c9-9984-4093-8914-3aa43c0e9440",
   "metadata": {},
   "source": [
    "Before I fit the model, I will be calling a stemming function from Porter Stemer (nltk). This will cut off word-ends to lower the word to its basic root. We will be adding this process to the \"tokenizer\" parameter of the vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942780ac-e6fc-41b4-8311-c37aba3f9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc802fe1-8dbb-4bfc-ae9d-84c46bf86e2a",
   "metadata": {},
   "source": [
    "### Removing Punctuation <a id=\"d1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7cd252-1e91-47a9-bbe2-d2542a1d68e7",
   "metadata": {},
   "source": [
    "This returns a list of common punctuations. We will include this into the tokenizer. We want to remove these as it does not really serve significant value to our predicted score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b230ff-32e6-414a-baff-83a9d71dd702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# import string & lists punctuations\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943cd487-d393-445a-accb-ea3a576e3f34",
   "metadata": {},
   "source": [
    "This returns a list of the most common punctuations. This is to show me what types will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c243fa-f5dc-4df6-9c1e-cfad0261dff2",
   "metadata": {},
   "source": [
    "### Stopwords <a id=\"d1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26c74143-ca79-403c-953d-01234e950540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/laerivera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8423589-8f41-45c6-ba35-4b0dae780bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of English Stop Words:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"List of English Stop Words:\")\n",
    "print(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d835cb-3170-4539-b6aa-5408a688beb1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4cb7f-0bb7-41dd-9221-bbf85e84698f",
   "metadata": {},
   "source": [
    "### Final Tokenizer <a id=\"d1.4\"></a>\n",
    "\n",
    "This tokenizer will be included during vectorization that will run my steps accordingly: remove punctuation, set letters to lowercase, split the sentences into words, remove stop words, stem the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1298efb5-58ab-4525-a69d-15a611cf9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    " def my_tokenizer(sentence):\n",
    "    # remove punctuation and set to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "    # split sentence into words\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "    \n",
    "    # remove stopwords and any tokens that are just empty strings\n",
    "    for word in listofwords:\n",
    "        if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "            # Stem words\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "    return listofstemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667dcf2a-938f-4db1-a4b4-14ba669a4789",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b75e9-0351-4d95-a41d-30f682e061a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Dependent/Independent Variables  <a id=\"d2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0ceac6-f40f-45d8-aa50-146f1e3f27b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = games['summary']\n",
    "y = games['meta_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5af3e710-ac07-4e8b-a4ea-7dece950a6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (17776,)\n",
      "y Shape: (17776,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X Shape: {X.shape}\")\n",
    "print(f\"y Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b1ca84e-33a6-4a1c-8f48-555f06248276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        As a young boy, Link is tricked by Ganondorf, ...\n",
       "1        As most major publishers' development efforts ...\n",
       "2         What does the American Dream mean today? For ...\n",
       "3        This is a tale of souls and swords, transcendi...\n",
       "4         What does the American Dream mean today? For ...\n",
       "                               ...                        \n",
       "17771    Fast & Furious: Showdown takes some of the fra...\n",
       "17772    Drake is out for revenge in a supernatural Hon...\n",
       "17773    Head out on a journey of redemption, driven by...\n",
       "17774     It has been 5 years since the outbreak, givin...\n",
       "17775    The Leisure Suit Larry: Box Office Bust video ...\n",
       "Name: summary, Length: 17776, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c68cfc0-8026-4945-ab80-e44a9f1d0375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9c4f7-c284-4e72-8817-28da03c55dca",
   "metadata": {},
   "source": [
    "My X/dependent variable will be the 'summary' as I am going to do some natural language processing. The y/independent variable will be the 'meta score'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac83e64-028c-4d84-99d3-9992890af327",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e8a93-0ac1-4f8b-868f-880887f131d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train/Test Split <a id=\"d2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be4053-4bcf-4aae-8450-784e05b5abc2",
   "metadata": {},
   "source": [
    "To begin our preparing for NLP machine learning models, I need to split my test and train data before vectorizing it. The split is done to identify our train and test data, which the model will be fitted and evaluated on. This is an extremely important step to the machine learning process. It ensures that overfitting is reduced and that our model can generlize itself better to new data that is introduced. This is important especially if this model will be used in real use cases. Splitting is also done before vectorizing the text data to prevent data leaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe09b22-a12f-4ac0-85c8-84a63db52194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting test size to 20%, and training at 80%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455b5d4f-cce2-4e2c-8aaa-b5d10147e662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: (14220,)\n",
      "X_test Shape: (3556,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train Shape: {X_train.shape}\")\n",
    "print(f\"X_test Shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60fa7892-5ae9-49ab-aed1-00b6bc6e549c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10851    TEAM-BASED SPACE COMBAT - Select from a variet...\n",
       "4178     A new generation of Pokémon has come to the Ni...\n",
       "5887     Age of Wonders: Planetfall is the new strategy...\n",
       "129      Master deep, tactical combat. Join up to 3 oth...\n",
       "8629     Chariot is a humoristic physics-based couch co...\n",
       "                               ...                        \n",
       "10955    A turn-based tactical RPG about a group of tro...\n",
       "17289    Take on the roll of Scarlett as you develop an...\n",
       "5192     Tensions mount for Mario and pals as each decl...\n",
       "12172    Taking a small step into the future, Red River...\n",
       "235      Drop in and conquer the massive, living mounta...\n",
       "Name: summary, Length: 14220, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at Xtrain\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fd553a0-01de-40f1-9bfc-f3f1af844bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train Shape: (14220,)\n",
      "y_test Shape: (3556,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_train Shape: {y_train.shape}\")\n",
    "print(f\"y_test Shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e97ce-9c91-4156-9bab-262073851316",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Distribution: Target Variable <a id=\"d2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb2a305e-1b1f-45e8-8f9f-9fa0f736ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution for y sets\n",
    "y_train_dist = round(y_train.value_counts(normalize=True).sort_index()*100, 2)\n",
    "y_test_dist = round(y_test.value_counts(normalize=True).sort_index()*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b82b36-2fff-4987-9d6f-eaa506eed20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the datatypes\n",
    "type(y_train_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9d56d7f-aef8-45bb-9078-32dd2d720d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the y Distribution\n",
    "plt.subplots(1, 2, figsize=(16,6))  # one row, two columns (1x2 grid)\n",
    "\n",
    "plt.subplot(1, 2, 1)  # slot 1\n",
    "y_train_d = y_train_dist.plot.bar(color=['red', 'blue'])\n",
    "plt.title('y_train Distribution', fontsize=20)\n",
    "plt.xlabel('Meta Score', fontsize=15)\n",
    "plt.ylabel('Distribution (%)', fontsize=15)\n",
    "plt.xticks(rotation = 360)\n",
    "plt.bar_label(y_train_d.containers[0], size=14)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)  # in my 1x2 grid of plots, what's in slot 2?\n",
    "y_test_d = y_test_dist.plot.bar(color=['red', 'blue'])\n",
    "plt.title('y_test Distribution', fontsize=20)\n",
    "plt.xlabel('Meta Score', fontsize=15)\n",
    "plt.ylabel('Distribution (%)', fontsize=15)\n",
    "plt.xticks(rotation = 360)\n",
    "plt.bar_label(y_test_d.containers[0], size=14)\n",
    "\n",
    "# comment out the following line and run cell to see the difference it makes\n",
    "plt.tight_layout()  # makes sure there is no overlap in plots\n",
    "\n",
    "plt.savefig('data/modelydists.jpg') #save file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06490d76-49cc-429c-ad3c-3bf81fd76080",
   "metadata": {},
   "source": [
    "As we can see, the distribution for the y_test and y_train after splitting is still very balanced. This is very good news since a balance in the target variable ensures there are no biases when the model is making predictions (prefers one class over the other). I will not need to deal with class imbalance later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aaecb1-ef8c-489d-b382-18a9b80e4d8e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f770c0d6-20f6-4cf3-81a9-d44c5a530763",
   "metadata": {},
   "source": [
    "# Vectorizers Used <a id=\"d3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe83476-bb55-4ced-b067-8fbead464f1c",
   "metadata": {},
   "source": [
    "\n",
    "**TF-IDF Vectorizer**: TF-IDF is composed of two different steps:\n",
    "1. Term Frequency (TF): This essentially returns the number of occurences a word has in a document (like document-term matrix);\n",
    "2. Inverse Document Frequency (IDF): This returns the inverse of the number of documents the word occurs in.\n",
    "\n",
    "Basically, words that are more common across the entire document will have a scaled-down count while words that are less common across the document will have a scaled-up count. Words that are less common will be focused on when trying to make predictions.\n",
    "\n",
    "**Count-Vectorizer (Bag-of-Words)**: is a type of tokenizer where each individual word in the corpus is counted by the number of times it occurs in each document. The result is a document-term matrix, where each column is each unique word (token) and the rows will correspond to the document.\n",
    "\n",
    "\n",
    "Sources:\n",
    "\n",
    "[TF-IDF Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "[Count-Vectorizer](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b09c46-fc75-44c2-abcc-fa5c6def9c87",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Establishing Performance Baseline <a id=\"d4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d977779-baf1-4397-beb7-640a590a694e",
   "metadata": {},
   "source": [
    "**Since the distribution of my predicted variable was already well-balanced, I am not worried about class imbalance, and thus using the **accuracy** score of the models instead of *f1 score* will be used for scoring.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40dfebf-e19d-4dba-9a4c-aa3b4df7ebde",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dummy Classifier (TF-IDF) <a id=\"d4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f74467-f091-4e8c-b52b-0437f8747e47",
   "metadata": {},
   "source": [
    "To assess the performance of later models done, I want to establish a *baseline* that I can compare future models to. I will be using a **Dummy Classifier** since it will attempt to make predictions without looking for patterns. A balance in class (that I mentioned earlier) ensures there are no biases when the model is making predictions.\n",
    "\n",
    "\n",
    "**Documentation and Sources**: \n",
    "\n",
    "[Dummy Classifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)\n",
    "\n",
    "[*Why Dummy Classifier is Important*](https://stackoverflow.com/questions/29441943/what-is-the-theorical-foundation-for-scikit-learn-dummy-classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748dda25-711c-4a7c-9d80-2390e3def5b6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51c480-b0fd-428e-9fc0-74dea2356149",
   "metadata": {},
   "source": [
    "**Separate TF-IDF Vectorizer only for Dummy Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ece2a1-3b3b-4f84-818c-743b17cb6757",
   "metadata": {},
   "source": [
    "Since I do not want to include my Dummy Classifier in my GridSearchCV (to keep things separated), I will be vectorizing the data separately. I will implement a min_df of 5 as well as implement my custom tokenizer created earlier in the notebook.\n",
    "\n",
    "*Note*: There is a warning saying the \"token_pattern will not be used\". This is normal because I used my own tokenizer. I can ignore this without problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42d0b2b8-2eb2-4b52-b535-6d77be169a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laerivera/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# Note: I do not want the dummy in the pipeline so I will create a separate vectorizer\n",
    "\n",
    "# Instantiate tfidf vectorizer for dummy only\n",
    "tfidf_dummy = TfidfVectorizer(min_df=5, # min_df is set to reduce noise\n",
    "                        tokenizer=my_tokenizer) # my custom tokenizer\n",
    "\n",
    "# Fit tfidf vectorizer\n",
    "tfidf_dummy.fit(X_train)\n",
    "\n",
    "# Transform\n",
    "X_train_dummy_t = tfidf_dummy.transform(X_train)\n",
    "X_test_dummy_t = tfidf_dummy.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6319ca08-2e7b-4b6d-acb2-53d278d2fd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of X_train Dummy: (14220, 9400)\n"
     ]
    }
   ],
   "source": [
    "print(f\" Shape of X_train Dummy: {X_train_dummy_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9de818b1-bc49-4d55-b77d-5ec20a2c0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dummy classifier\n",
    "baseline = DummyClassifier() \n",
    "baseline.fit(X_train_dummy_t, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_baseline_t = baseline.predict(X_test_dummy_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3171d-fc78-4080-8934-4b7a82c91457",
   "metadata": {},
   "source": [
    "### Dummy Results <a id=\"d4.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0df69f4e-bd30-44bf-a869-805648651ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Accuracy Score (%): 51.07\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy score\n",
    "print(f\"Dummy Accuracy Score (%): {round(accuracy_score(y_test, y_pred_baseline_t)*100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c168f9d-cc2c-4fc6-8591-115274603e51",
   "metadata": {},
   "source": [
    "**Dummy Classification Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099ebbf-1970-43f8-96bb-ea624add3938",
   "metadata": {},
   "source": [
    "Although I will not be using a classification report, and will merely use the accuracy score, I will use it here for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1eb10b5-72b4-4bd2-9dbd-9544384401aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: Dummy Classifier \n",
      "=======================================================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1740\n",
      "           1       0.51      1.00      0.68      1816\n",
      "\n",
      "    accuracy                           0.51      3556\n",
      "   macro avg       0.26      0.50      0.34      3556\n",
      "weighted avg       0.26      0.51      0.35      3556\n",
      "\n",
      "\n",
      "=======================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laerivera/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/laerivera/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/laerivera/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Classification Report \n",
    "\n",
    "print_line = \"\\n=======================================================\\n\"\n",
    "print(\"Classification Report: Dummy Classifier\", print_line)\n",
    "class_report_baseline = classification_report(y_test, y_pred_baseline_t)\n",
    "print(class_report_baseline)\n",
    "print(print_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0129a3-2c28-455c-8fe7-c028662b7d9b",
   "metadata": {},
   "source": [
    "**Dummy Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcd837f9-e339-4e56-96bd-52cc44bbb85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fca12a63190>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(baseline, X_test_dummy_t, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44688a9c-ccdf-4247-b834-d6872d965974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    48.78\n",
       "1    51.22\n",
       "Name: meta_score, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dummy Distribution\")\n",
    "dummy_train = round(y_train.value_counts(normalize=True).sort_index()*100, 2)\n",
    "dummy_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f457f3b-1645-4101-9605-fd024e830d41",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Based on the confusion matrix, we can see that the dummy classifier predicted all '1'/'good' game scores. This gives it a **0.51 (51%) accuracy score**. This is consistent with the distribution of prediction of y='1'. This shows that the if the dummy predicted all 1's it will always get an accuracy score of 0.51. \n",
    "\n",
    "Models will aim to be above this 51% accuracy score. This ensures that the models are operating better than merely random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47ac18-6074-4458-ae5a-32bdfc164d66",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143a630-ebb0-4001-8235-4a01e6f32a4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper Function: Model File Loader <a id=\"d5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c5509-c4f1-47e0-9e3f-c400dc24daed",
   "metadata": {},
   "source": [
    "Due to the long execution times, I will use a customized helper function by [Andrew Dang](https://github.com/andrew-dang) to reduce running models when it is already saved in a `.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bc2000f-bce3-4819-a43d-568daeb4dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fitted_models():\n",
    "    '''\n",
    "    Function that loads fitted models and sets the model_loaded_flag to True. \n",
    "    Saves readers the trouble of having to fit all the models every time they open the notebook.\n",
    "    \n",
    "    INPUT:\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    OUTPUT:\n",
    "    -------\n",
    "    model_dict: a dictionary that contain fitted models\n",
    "    models_loaded_flag: A boolean. If set to True, most models in the notebook will not undergo \n",
    "                        fitting, and load models from the dictionary instead.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    model_dict = {} # Filling in dictionary with models\n",
    "    model_dict['gs1'] = joblib.load('data/fitted_gridsearchcv1.pkl')\n",
    "    model_dict['gs2'] = joblib.load('data/fitted_gridsearchcv2.pkl')\n",
    "    model_dict['gs3'] = joblib.load('data/fitted_gridsearchcv3.pkl')\n",
    "    model_dict['gs4'] = joblib.load('data/fitted_gridsearchcv4.pkl')\n",
    "    \n",
    "    models_loaded_flag = True # set to false if re-executing the training of models\n",
    "    return model_dict, models_loaded_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ab93ba3-0a6d-4d35-b484-bdf77254f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fitted models\n",
    "model_dict, models_loaded_flag = load_fitted_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27bddd5-234b-43fa-a3f4-f62776d37c80",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ea61d-6ab6-4224-b748-f722004705d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modeling <a id=\"d6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97fdf6-2fe7-47fb-bcc8-194eeb174a85",
   "metadata": {},
   "source": [
    "Now it's time to use GridSearchCV to find the most optimal model and hyperparameter. \n",
    "- *Note*: Standard and MinMax Scalers will *not* be done. Since most of my data is binary, implementing a scaler will be redundant and inappropriate. \n",
    "\n",
    "**Classifiers to run**:\n",
    "1. Logistic Classifier\n",
    "2. KNN Classifiers\n",
    "4. Decision Tree Classifier\n",
    "5. Random Forest Classifier\n",
    "\n",
    "***\n",
    "**Workflow**:\n",
    "\n",
    "[**Grid Search 1: Control**](#d7)<br>\n",
    "In GridSearch 1, I will be implementing Logistic, K-NearestNeighbors , Decision Tree, and Random Forest Classifiers. The results will be analyzed and I will proceed with the modeling process until the results are conclusive and satisfactory\n",
    "\n",
    "\n",
    "[**Grid Search 2: Ngrams**](#d8)<br>\n",
    "Based on the results of GridSearch 1, I will implement N-grams (specifically bigrams) to the analysis to observe how it affects the model performance.\n",
    "\n",
    "\n",
    "[**Grid Search 3: Feature Engineering**](#d9)<br>\n",
    "Based on the results of GridSearch 2, I will do some feature engineering to observe the affects the model performance.\n",
    "\n",
    "\n",
    "[**Grid Search 4: Feature Engineering - Expand Parameters**](#d10)<br>\n",
    "Based on the results of GridSearch 3, I will replicate GridSearch 3 and widen the parameter range to get the most optimized result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada9e09f-3793-4178-a953-8415aa1e6950",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529f572-bf26-4d96-9f2b-10d6b9f0045e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GridSearch 1 - The Control <a id=\"d7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7d1de-b5d4-40c3-b023-d1c0dd8a5c52",
   "metadata": {},
   "source": [
    "\n",
    "**Fitted Models**:\n",
    "- Logistic Classifier\n",
    "- KNN Classifier\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "\n",
    "**Vectorizers**:\n",
    "- TF-IDF, Count Vectorizers\n",
    "\n",
    "**Run-time**: 21 minutes\n",
    "\n",
    "***Notes GridSearch 1***:\n",
    "- The `C` hyperparameter affects regularization (penalty)-where the smaller 'C' is, the stronger regularization will be applied. The warnings merely indicate that when the penalty is set to 'None', then the C will be ignored. \n",
    "- Max_iter is specified to allow the logistic regression to converge.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b906dc-8526-47d5-910a-fd103499349f",
   "metadata": {},
   "source": [
    "**GridSearch 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a33bbd6c-fd86-4dfa-a939-ae721c754600",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "\n",
      "The best model used the following settings: \n",
      " Pipeline(steps=[('vectorizer',\n",
      "                 TfidfVectorizer(min_df=5,\n",
      "                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>)),\n",
      "                ('model', KNeighborsClassifier(n_neighbors=3))])\n",
      "Best Model Train Score (%): 95.69\n",
      "Best Model Test Score (%): 71.23\n"
     ]
    }
   ],
   "source": [
    "if models_loaded_flag: # SOURCE: Andrew Dang\n",
    "    print('Loading pre-trained model...')\n",
    "    gs1 = model_dict['gs1'] # reference the pkl if flag is set to True\n",
    "    \n",
    "    # Print the best estimator\n",
    "    print('\\nThe best model used the following settings:', '\\n',gs1.best_estimator_)\n",
    "    \n",
    "    # Print score \n",
    "    print(f\"Best Model Train Score (%): {round(gs1.score(X_train, y_train)*100.00, 2)}\") #train score\n",
    "    print(f\"Best Model Test Score (%): {round(gs1.score(X_test, y_test)*100.00,2)}\") # test score\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "else: \n",
    "\n",
    "    # State Models will re-train\n",
    "    print(\"Training Models...\")\n",
    "\n",
    "    # GRIDSEARCH 1: \n",
    "\n",
    "    estimators1 = [('vectorizer', TfidfVectorizer()),\n",
    "                ('model', LogisticRegression())]\n",
    "\n",
    "\n",
    "    pipe1 = Pipeline(estimators1)\n",
    "\n",
    "    # Instantiate Pipeline with the specified steps\n",
    "    param_grid1 = [\n",
    "\n",
    "    # Logistic Classifier\n",
    "                {'vectorizer': [TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), \n",
    "                                CountVectorizer(min_df=5, tokenizer=my_tokenizer)],\n",
    "                'model': [LogisticRegression()],\n",
    "                'model__C': [0.001, 0.01, 0.1, 1, 10], # C parameters\n",
    "                'model__penalty': ['l2', 'none'],  # penalty applied\n",
    "                'model__max_iter': [10000], # max iterations is set to higher value to make sure lines converge\n",
    "                'model__random_state': [1]}, # random state\n",
    "    # # KNN\n",
    "                {'vectorizer': [TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), \n",
    "                                CountVectorizer(min_df=5, tokenizer=my_tokenizer)],\n",
    "                 'model': [KNeighborsClassifier()],\n",
    "                 'model__n_neighbors': range(3, 10)},\n",
    "\n",
    "    # # Decision Tree Classifier\n",
    "                {'vectorizer': [TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), \n",
    "                                CountVectorizer(min_df=5, tokenizer=my_tokenizer)],\n",
    "                'model': [DecisionTreeClassifier()],\n",
    "                'model__max_depth': range(2, 6), #limits number of splits made/complexity\n",
    "                'model__min_samples_leaf':[2], #places lower bound on number of datapoints each region covers\n",
    "                'model__random_state': [1]},\n",
    "\n",
    "    # # Random Forest\n",
    "                {'vectorizer': [TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), \n",
    "                                CountVectorizer(min_df=5, tokenizer=my_tokenizer)],\n",
    "                 'model': [RandomForestClassifier()],\n",
    "                 'model__max_depth': range(2, 6),\n",
    "                 'model__min_samples_leaf':[2],\n",
    "                 'model__random_state': [1]},\n",
    "                    ]\n",
    "\n",
    "    # Instate cross-validated grid search object\n",
    "    grid1 = GridSearchCV(\n",
    "        estimator = pipe1, # define estimator\n",
    "        param_grid = param_grid1, # define parameter \n",
    "        cv = 5, # cross validation folds\n",
    "        verbose = 0,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearch\n",
    "    fitted_grid1 = grid1.fit(X_train, y_train) # Fit grid to train sets\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------#    \n",
    "    \n",
    "    # Saving GridSearch 1 as a pickle file\n",
    "    grid_search_model = fitted_grid1\n",
    "    joblib.dump(fitted_grid1, 'data/fitted_gridsearchcv1.pkl')\n",
    "    # Setting variable\n",
    "    gs1 = joblib.load('data/fitted_gridsearchcv1.pkl' ) # NOTE: Unusual, but doing the loading first before calling best estimator/scores since run times are quicker for me\n",
    "    \n",
    "    \n",
    "    # Best Estimator of GridSearch\n",
    "    print(gs1.best_estimator_)\n",
    "    \n",
    "    # Print Score\n",
    "    print(f\"Best Model Train Score (%): {round(gs1.score(X_train, y_train)*100.00, 2)}\") # best train score\n",
    "    print(f\"Best Model Test Score (%): {round(gs1.score(X_test, y_test)*100.00,2)}\") # best test score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ee2b6-b537-427a-9295-ea089399da47",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ce3af-0436-409b-a6ee-6692b4892cef",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GridSearch 1 Results <a id=\"d7.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4511e8a0-d36a-4a3d-92a5-5ede41e671fc",
   "metadata": {},
   "source": [
    "**GridSearch 1**\n",
    "\n",
    "Here, the Top 10 Scores (ranked by mean test score) are organized into a dataframe. This allows me to view the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdcc43e8-f37c-49ab-8aeb-f8fc4630cf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch 1 Cross Validation Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>{'model': KNeighborsClassifier(n_neighbors=3), 'model__n_neighbors': 3, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.684529</td>\n",
       "      <td>0.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>{'model': LogisticRegression(), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.675035</td>\n",
       "      <td>0.010733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>{'model': LogisticRegression(), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.673840</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(), 'model__C': 0.001, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672925</td>\n",
       "      <td>0.007310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672925</td>\n",
       "      <td>0.007310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(), 'model__C': 0.01, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672925</td>\n",
       "      <td>0.007310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672925</td>\n",
       "      <td>0.007310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(), 'model__C': 0.1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672925</td>\n",
       "      <td>0.007310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>{'model': LogisticRegression(), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672855</td>\n",
       "      <td>0.008660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>{'model': LogisticRegression(), 'model__C': 0.1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.667792</td>\n",
       "      <td>0.008787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_score  \\\n",
       "20                1   \n",
       "13                2   \n",
       "16                3   \n",
       "2                 4   \n",
       "18                4   \n",
       "6                 4   \n",
       "14                4   \n",
       "10                4   \n",
       "17                9   \n",
       "11               10   \n",
       "\n",
       "                                                                                                                                                                                                                            params  \\\n",
       "20                                                           {'model': KNeighborsClassifier(n_neighbors=3), 'model__n_neighbors': 3, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "13        {'model': LogisticRegression(), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "16       {'model': LogisticRegression(), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "2   {'model': LogisticRegression(), 'model__C': 0.001, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "18     {'model': LogisticRegression(), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "6    {'model': LogisticRegression(), 'model__C': 0.01, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "14      {'model': LogisticRegression(), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "10    {'model': LogisticRegression(), 'model__C': 0.1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "17       {'model': LogisticRegression(), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "11    {'model': LogisticRegression(), 'model__C': 0.1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "\n",
       "    mean_test_score  std_test_score  \n",
       "20         0.684529        0.020900  \n",
       "13         0.675035        0.010733  \n",
       "16         0.673840        0.012000  \n",
       "2          0.672925        0.007310  \n",
       "18         0.672925        0.007310  \n",
       "6          0.672925        0.007310  \n",
       "14         0.672925        0.007310  \n",
       "10         0.672925        0.007310  \n",
       "17         0.672855        0.008660  \n",
       "11         0.667792        0.008787  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None) #set dataframes with expanded columns to view full parameters\n",
    "\n",
    "# Show results in a dataframe\n",
    "gs1_results_df = pd.DataFrame(gs1.cv_results_).sort_values('mean_test_score', ascending=False) #rank by mean_test_score\n",
    "gs1_results = gs1_results_df[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False) # Order by highest mean_test_score/5 fold cross validation\n",
    "print(\"GridSearch 1 Cross Validation Results\")\n",
    "gs1_results.head(10) # top 10 cv results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b54232d-cdf3-40b9-a0b0-3dc38ad6ab75",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f87ae3-ed63-4bc0-a320-cdb174eb1d57",
   "metadata": {},
   "source": [
    "#### GridSearch 1 Summary <a id=\"d7.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d943b32-5c4c-4be1-bf47-6a6177555138",
   "metadata": {},
   "source": [
    "- GridSearch 1 gave us a Best Model Train Score of 95% and a test score of **71.23%**. These scores show that the model has trained very well but did not generalize well with the unseen (test) data. This means the model is **over-fitting** quite a bit despite applying a 5-fold cross validation. However, it is good to note that this is roughly 20% higher than that of the dummy score of 51%.\n",
    "\n",
    "\n",
    "- The main purpose of my project is to predict whether a game has a \"high\" or \"low\" meta score based on the game characteristics. However, due to the nature of the optimal model for GS1, the KNN Classifier is **difficult to interpret**. For example, with a Logistic Regression, the tokens itself are attached to coefficients that would tell me where the features contribute more to a high or low score. The same cannot be done with a KNN model. With a KNN model, I will not necessarily know the relationship between the words to a game score since the way it measures is by a type of \"distance\" (not coefficient) Due to the **conflicting purpose of the best model and the purpose of my business question**, I will remove KNN from future models/GridSearch.\n",
    "\n",
    "- Furthermore, when looking at the cross validation scores outlined in \"GridSearch 1 Cross Validation Scores\", we can see a higher amount of standard test scores for the most optimal KNN compared to the most optimal Logistic Regression classifier. In other words, the KNN model has more volatility. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b10793f-7bca-44fb-9c2c-4568f9be5736",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6363b-bd9a-4ce8-9d75-b80f18ce631a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GridSearch 2 - Ngrams <a id=\"d8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1899d7-4abc-4f8c-b82b-48199f77c735",
   "metadata": {},
   "source": [
    "After fitting my first GridSearch and determining that KNN was difficult to interpret in regard to my project purpose, I have removed it for GridSearch 2. In this GridSearch, I will be adding Ngrams to the vectorizer. Ngrams are essentially pairs of consecutive words that help maintain the sequence and interpretability of the token. In this case, I will apply a bigram (2-gram).\n",
    "\n",
    "**Fitted Models**:\n",
    "- Logistic Classifier\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "\n",
    "**Vectorizers**:\n",
    "- TF-IDF, Count Vectorizers, **Bigrams** \n",
    "\n",
    "***Runtime:***\n",
    "- 40 minutes\n",
    "\n",
    "**Source**:\n",
    "\n",
    "[Ngrams](https://pythonhosted.org/ngram/ngram.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db1872-763b-420c-ba33-753211ed8950",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42d209-169b-4ebf-92ea-fbdef02d3f35",
   "metadata": {},
   "source": [
    "**GridSearch 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "520706a9-5718-4cea-9b02-a402f309fd92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "\n",
      "The best model used the following settings: \n",
      " Pipeline(steps=[('vectorizer',\n",
      "                 CountVectorizer(min_df=5,\n",
      "                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>)),\n",
      "                ('model',\n",
      "                 LogisticRegression(C=1, max_iter=10000, random_state=1))])\n",
      "Best Model Train Score (%): 91.65\n",
      "Best Model Test Score (%): 68.81\n"
     ]
    }
   ],
   "source": [
    "if models_loaded_flag:  #Helper Function\n",
    "    print('Loading pre-trained model...')\n",
    "    gs2 = model_dict['gs2'] # reference the pkl\n",
    "    \n",
    "    # Print the best estimator\n",
    "    print('\\nThe best model used the following settings:', '\\n',gs2.best_estimator_)\n",
    "    \n",
    "    # Print score \n",
    "    print(f\"Best Model Train Score (%): {round(gs2.score(X_train, y_train)*100.00, 2)}\")\n",
    "    print(f\"Best Model Test Score (%): {round(gs2.score(X_test, y_test)*100.00,2)}\")\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "else:\n",
    "    \n",
    "    # State Models will re-train\n",
    "    print(\"Training Models...\")\n",
    "\n",
    "    # GRIDSEARCH 2:\n",
    "\n",
    "    estimators2 = [('vectorizer', TfidfVectorizer()),\n",
    "                ('model', LogisticRegression())]\n",
    "\n",
    "\n",
    "    pipe2 = Pipeline(estimators2)\n",
    "\n",
    "    # Instantiate Pipeline with the specified steps\n",
    "    param_grid2 = [\n",
    "\n",
    "    # Logistic Classifier\n",
    "                {'vectorizer': [TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), \n",
    "                                CountVectorizer(min_df=5, tokenizer=my_tokenizer),\n",
    "                                TfidfVectorizer(min_df=5, tokenizer=my_tokenizer, ngram_range=(2, 2)), # adding bigrams\n",
    "                                CountVectorizer(min_df=5, tokenizer=my_tokenizer, ngram_range=(2, 2))\n",
    "                               ],\n",
    "                'model': [LogisticRegression()],\n",
    "                'model__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                'model__penalty': ['l2', 'none'],\n",
    "                'model__max_iter': [10000],\n",
    "                'model__random_state': [1]},\n",
    "\n",
    "    # # Decision Tree Classifier\n",
    "                {'vectorizer': [TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), \n",
    "                                CountVectorizer(min_df=5, tokenizer=my_tokenizer),\n",
    "                               TfidfVectorizer(min_df=5, tokenizer=my_tokenizer, ngram_range=(2, 2)), # adding bigrams\n",
    "                               CountVectorizer(min_df=5, tokenizer=my_tokenizer, ngram_range=(2, 2))\n",
    "                               ],\n",
    "                'model': [DecisionTreeClassifier()],\n",
    "                'model__max_depth': range(2, 6), #limits number of splits made/complexity\n",
    "                'model__min_samples_leaf':[2], #places lower bound on number of datapoints each region covers\n",
    "                'model__random_state': [1]},\n",
    "\n",
    "    # # Random Forest\n",
    "                {'vectorizer': [TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), \n",
    "                                CountVectorizer(min_df=5, tokenizer=my_tokenizer),\n",
    "                               TfidfVectorizer(min_df=5, tokenizer=my_tokenizer, ngram_range=(2, 2)), # adding bigrams\n",
    "                                CountVectorizer(min_df=5, tokenizer=my_tokenizer, ngram_range=(2, 2))\n",
    "                               ],\n",
    "                 'model': [RandomForestClassifier()],\n",
    "                 'model__max_depth': range(2, 6),\n",
    "                 'model__min_samples_leaf':[2],\n",
    "                 'model__random_state': [1]},\n",
    "                    ]\n",
    "\n",
    "    # Instate cross-validated grid search object\n",
    "    grid_2 = GridSearchCV(\n",
    "        estimator = pipe2,\n",
    "        param_grid = param_grid2,\n",
    "        cv = 5, # cross validation folds\n",
    "        verbose = 0,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearch \n",
    "    fitted_grid2 = grid_2.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------# \n",
    "    \n",
    "    # Saving GridSearch 2 as a pkl\n",
    "    gs2 = fitted_grid2\n",
    "    joblib.dump(fitted_grid2, 'data/fitted_gridsearchcv2.pkl')\n",
    "    # Setting variable\n",
    "    gs2 = joblib.load('data/fitted_gridsearchcv2.pkl' ) # NOTE: Unusual, but doing the loading first before calling best estimator/scores since run times are quicker for me\n",
    "    \n",
    "    \n",
    "    # Best Estimator of GridSearch\n",
    "    print(gs2.best_estimator_)\n",
    "    \n",
    "    # Print Score\n",
    "    print(f\"Best Model Train Score (%): {round(gs2.score(X_train, y_train)*100.00, 2)}\")\n",
    "    print(f\"Best Model Test Score (%): {round(gs2.score(X_test, y_test)*100.00,2)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55086ce-4203-4484-9d16-67f58be3c2e8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716e248-bf5c-41c2-97bd-09950e274bbe",
   "metadata": {},
   "source": [
    "#### GridSearch 2 Results <a id=\"d8.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63e1d477-a02f-4087-9dd8-e33b28e92d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch 2 Cross Validation Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>{'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.675035</td>\n",
       "      <td>0.010733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>{'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, ngram_range=(2, 2),\n",
       "                tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.674191</td>\n",
       "      <td>0.006744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>{'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.673840</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, ngram_range=(2, 2),\n",
       "                tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672996</td>\n",
       "      <td>0.005850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5</td>\n",
       "      <td>{'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672925</td>\n",
       "      <td>0.007310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>{'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 0.01, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672925</td>\n",
       "      <td>0.007310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>{'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 0.001, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672925</td>\n",
       "      <td>0.007310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>{'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672925</td>\n",
       "      <td>0.007310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>{'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 0.1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672925</td>\n",
       "      <td>0.007310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10</td>\n",
       "      <td>{'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;)}</td>\n",
       "      <td>0.672855</td>\n",
       "      <td>0.008660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_score  \\\n",
       "25                1   \n",
       "34                2   \n",
       "32                3   \n",
       "27                4   \n",
       "36                5   \n",
       "12                5   \n",
       "4                 5   \n",
       "28                5   \n",
       "20                5   \n",
       "33               10   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                              params  \\\n",
       "25                                       {'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "34  {'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, ngram_range=(2, 2),\n",
       "                tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "32                                      {'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "27   {'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, ngram_range=(2, 2),\n",
       "                tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "36                                    {'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "12                                  {'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 0.01, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "4                                  {'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 0.001, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "28                                     {'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "20                                   {'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 0.1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'vectorizer': TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "33                                      {'model': LogisticRegression(C=1, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'vectorizer': CountVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>)}   \n",
       "\n",
       "    mean_test_score  std_test_score  \n",
       "25         0.675035        0.010733  \n",
       "34         0.674191        0.006744  \n",
       "32         0.673840        0.012000  \n",
       "27         0.672996        0.005850  \n",
       "36         0.672925        0.007310  \n",
       "12         0.672925        0.007310  \n",
       "4          0.672925        0.007310  \n",
       "28         0.672925        0.007310  \n",
       "20         0.672925        0.007310  \n",
       "33         0.672855        0.008660  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2_results_df = pd.DataFrame(gs2.cv_results_).sort_values('mean_test_score', ascending=False) #create df and arrange by score\n",
    "gs2_results = gs2_results_df[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False) # Order by highest mean_test_score/5 fold cross validation\n",
    "print(\"GridSearch 2 Cross Validation Results\")\n",
    "gs2_results.head(10) #first 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e838b-716d-42b5-8b9f-10241f04cc9c",
   "metadata": {},
   "source": [
    "#### GridSearch 2 Summary <a id=\"d8.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1179a84-bb1f-4de3-ac3f-0271e62a26c1",
   "metadata": {},
   "source": [
    "- In GridSearch 2, where bigrams were added and the KNN model removed, the best model seemed to be a Logistic Regression Classifier, tokenized with Count Vectorizer, and with a C=1. This model received a **train score of 91.65% and a test score of 68.81%**. Once again, the model is overfitting, which suggests the test is having difficulty generalizing to unknown data.\n",
    "\n",
    "- The best test score obtained in GridSearch 2 has reduced since GridSearch 1, especially with the removal of the KNN model. Although the score went from ~71% to a 68%, I maintain interpretability since it is a Logistic Regression Classifier with coefficients attached to the tokens.\n",
    "\n",
    "- In terms of bigrams, I observed that a model with bigrams was ranked 2nd in terms of cross validation score. This test score suggests that the bigrams have helped the model test a little bit, but not enough to be the best performing model——however, it's worth noting that the cross validation score between the best model and the second-best are off only by 0.001%. I was hoping that bigrams would significantly improve model performance, but I was mistaken. Had it suceeded, I would keep it to help me interpret my tokens in the end, but I will take bigrams out for GridSearch 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e8ba8-fed5-4bc6-914a-7d980a29fbcf",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dfe2d4-6154-4138-98be-31460ff0daff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GridSearch 3 - Feature Engineering <a id=\"d9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3d097-7b2e-40d2-8b60-ea52fad3450e",
   "metadata": {},
   "source": [
    "After fitting GridSearch1 and 2, it is clear that the model is overfitting quite largely. I will then exhaust all avenues of improving the test accuracy score in later steps. My plan in GridSearch3 is to add some of the features I discarded earlier and see if that improves the performance of my model. If the features improve the test score, I will keep the new features for my final GridSearch, and if not, I will remove this in the next steps. \n",
    "\n",
    "**Fitted Models**:\n",
    "- Logistic\n",
    "- DecisionTree\n",
    "- RandomForest Classifiers\n",
    "    \n",
    "**Vectorizers**:\n",
    "- TF-IDF, Count Vectorizer, **Bigrams Removed**\n",
    "\n",
    "**Features**:\n",
    "- Added features include: `game of the year`, `platform`, and `user review` via ColumnTransformer()\n",
    "\n",
    "**Runtime**: 15 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796a854-c167-404a-bce4-ea3fac22b5f4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24075651-7f3b-4694-843c-edcbfbb86ee0",
   "metadata": {},
   "source": [
    "### Adding Features <a id=\"d9.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e99f5f-448c-492e-ad0c-84b58f8da906",
   "metadata": {},
   "source": [
    "These are the features I will include: \n",
    "\n",
    "- **Platform**: This would tell me if the platform-type of the game released would define whether a game's meta score is low/high.\n",
    "- **user_review**: User review is another scoring system done by the users. I can investigate whether this can determine a low/high meta score.\n",
    "- **game_of_the_year**: It would be interesting to see if receiving the MetaCritic's Game of the Year awards contributes highly to a low/high meta score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f476b-8630-4740-87ab-24c56c14f4aa",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b97213e-3fa8-48c8-a216-f7059eb069b7",
   "metadata": {},
   "source": [
    "**Create DataFrame Copy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "627cc172-eb6f-4a2b-ba27-2a21bdf50562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the clean dataframe\n",
    "games_2 = games_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "025fb08d-6b6c-43bc-a370-c878cf0d2f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>platform</th>\n",
       "      <th>summary</th>\n",
       "      <th>meta_score</th>\n",
       "      <th>user_review</th>\n",
       "      <th>release_year</th>\n",
       "      <th>release_month</th>\n",
       "      <th>game_of_the_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Legend of Zelda: Ocarina of Time</td>\n",
       "      <td>Nintendo 64</td>\n",
       "      <td>As a young boy, Link is tricked by Ganondorf, the King of the Gerudo Thieves. The evil human uses Link to gain access to the Sacred Realm, where he places his tainted hands on Triforce and transforms the beautiful Hyrulean landscape into a barren wasteland. Link is determined to fix the problems he helped to create, so with the help of Rauru he travels through time gathering the powers of the Seven Sages.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tony Hawk's Pro Skater 2</td>\n",
       "      <td>PlayStation</td>\n",
       "      <td>As most major publishers' development efforts shift to any number of next-generation platforms, Tony Hawk 2 will likely stand as one of the last truly fantastic games to be released on the PlayStation.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Theft Auto IV</td>\n",
       "      <td>PlayStation 3</td>\n",
       "      <td>What does the American Dream mean today? For Niko Belic, fresh off the boat from Europe. It's the hope he can escape his past. For his cousin, Roman, it is the vision that together they can find fortune in Liberty City, gateway to the land of opportunity. As they slip into debt and are dragged into a criminal underworld by a series of shysters, thieves and sociopaths, they discover that the reality is very different from the dream in a city that worships money and status, and is heaven for those who have them an a living nightmare for those who don't.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name        platform  \\\n",
       "0  The Legend of Zelda: Ocarina of Time     Nintendo 64   \n",
       "1              Tony Hawk's Pro Skater 2     PlayStation   \n",
       "2                   Grand Theft Auto IV   PlayStation 3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           summary  \\\n",
       "0                                                                                                                                                         As a young boy, Link is tricked by Ganondorf, the King of the Gerudo Thieves. The evil human uses Link to gain access to the Sacred Realm, where he places his tainted hands on Triforce and transforms the beautiful Hyrulean landscape into a barren wasteland. Link is determined to fix the problems he helped to create, so with the help of Rauru he travels through time gathering the powers of the Seven Sages.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                        As most major publishers' development efforts shift to any number of next-generation platforms, Tony Hawk 2 will likely stand as one of the last truly fantastic games to be released on the PlayStation.   \n",
       "2   What does the American Dream mean today? For Niko Belic, fresh off the boat from Europe. It's the hope he can escape his past. For his cousin, Roman, it is the vision that together they can find fortune in Liberty City, gateway to the land of opportunity. As they slip into debt and are dragged into a criminal underworld by a series of shysters, thieves and sociopaths, they discover that the reality is very different from the dream in a city that worships money and status, and is heaven for those who have them an a living nightmare for those who don't.    \n",
       "\n",
       "   meta_score  user_review  release_year  release_month game_of_the_year  \n",
       "0           1            1          1998             11                0  \n",
       "1           1            1          2000              9                0  \n",
       "2           1            1          2008              4                1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first few rows\n",
    "games_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca1c2aa0-b30d-4cfc-93ea-81096a29a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the columns I need only\n",
    "games_2 = games_2.drop(columns=['name', 'release_year', 'release_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f4293ca-acf8-4069-9d74-501f7f0c962f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>summary</th>\n",
       "      <th>meta_score</th>\n",
       "      <th>user_review</th>\n",
       "      <th>game_of_the_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nintendo 64</td>\n",
       "      <td>As a young boy, Link is tricked by Ganondorf, the King of the Gerudo Thieves. The evil human uses Link to gain access to the Sacred Realm, where he places his tainted hands on Triforce and transforms the beautiful Hyrulean landscape into a barren wasteland. Link is determined to fix the problems he helped to create, so with the help of Rauru he travels through time gathering the powers of the Seven Sages.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PlayStation</td>\n",
       "      <td>As most major publishers' development efforts shift to any number of next-generation platforms, Tony Hawk 2 will likely stand as one of the last truly fantastic games to be released on the PlayStation.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PlayStation 3</td>\n",
       "      <td>What does the American Dream mean today? For Niko Belic, fresh off the boat from Europe. It's the hope he can escape his past. For his cousin, Roman, it is the vision that together they can find fortune in Liberty City, gateway to the land of opportunity. As they slip into debt and are dragged into a criminal underworld by a series of shysters, thieves and sociopaths, they discover that the reality is very different from the dream in a city that worships money and status, and is heaven for those who have them an a living nightmare for those who don't.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         platform  \\\n",
       "0     Nintendo 64   \n",
       "1     PlayStation   \n",
       "2   PlayStation 3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           summary  \\\n",
       "0                                                                                                                                                         As a young boy, Link is tricked by Ganondorf, the King of the Gerudo Thieves. The evil human uses Link to gain access to the Sacred Realm, where he places his tainted hands on Triforce and transforms the beautiful Hyrulean landscape into a barren wasteland. Link is determined to fix the problems he helped to create, so with the help of Rauru he travels through time gathering the powers of the Seven Sages.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                        As most major publishers' development efforts shift to any number of next-generation platforms, Tony Hawk 2 will likely stand as one of the last truly fantastic games to be released on the PlayStation.   \n",
       "2   What does the American Dream mean today? For Niko Belic, fresh off the boat from Europe. It's the hope he can escape his past. For his cousin, Roman, it is the vision that together they can find fortune in Liberty City, gateway to the land of opportunity. As they slip into debt and are dragged into a criminal underworld by a series of shysters, thieves and sociopaths, they discover that the reality is very different from the dream in a city that worships money and status, and is heaven for those who have them an a living nightmare for those who don't.    \n",
       "\n",
       "   meta_score  user_review game_of_the_year  \n",
       "0           1            1                0  \n",
       "1           1            1                0  \n",
       "2           1            1                1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4723cde4-10a9-4879-b810-97b390b0d262",
   "metadata": {},
   "source": [
    "**Defining Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a1d5743-c502-4095-86d0-3baf0c4925cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X2 = games_2.drop(columns=['meta_score'])\n",
    "y2 = games_2['meta_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de8bc9b3-de02-4e90-951c-acf435052c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>summary</th>\n",
       "      <th>user_review</th>\n",
       "      <th>game_of_the_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nintendo 64</td>\n",
       "      <td>As a young boy, Link is tricked by Ganondorf, the King of the Gerudo Thieves. The evil human uses Link to gain access to the Sacred Realm, where he places his tainted hands on Triforce and transforms the beautiful Hyrulean landscape into a barren wasteland. Link is determined to fix the problems he helped to create, so with the help of Rauru he travels through time gathering the powers of the Seven Sages.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PlayStation</td>\n",
       "      <td>As most major publishers' development efforts shift to any number of next-generation platforms, Tony Hawk 2 will likely stand as one of the last truly fantastic games to be released on the PlayStation.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PlayStation 3</td>\n",
       "      <td>What does the American Dream mean today? For Niko Belic, fresh off the boat from Europe. It's the hope he can escape his past. For his cousin, Roman, it is the vision that together they can find fortune in Liberty City, gateway to the land of opportunity. As they slip into debt and are dragged into a criminal underworld by a series of shysters, thieves and sociopaths, they discover that the reality is very different from the dream in a city that worships money and status, and is heaven for those who have them an a living nightmare for those who don't.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         platform  \\\n",
       "0     Nintendo 64   \n",
       "1     PlayStation   \n",
       "2   PlayStation 3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           summary  \\\n",
       "0                                                                                                                                                         As a young boy, Link is tricked by Ganondorf, the King of the Gerudo Thieves. The evil human uses Link to gain access to the Sacred Realm, where he places his tainted hands on Triforce and transforms the beautiful Hyrulean landscape into a barren wasteland. Link is determined to fix the problems he helped to create, so with the help of Rauru he travels through time gathering the powers of the Seven Sages.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                        As most major publishers' development efforts shift to any number of next-generation platforms, Tony Hawk 2 will likely stand as one of the last truly fantastic games to be released on the PlayStation.   \n",
       "2   What does the American Dream mean today? For Niko Belic, fresh off the boat from Europe. It's the hope he can escape his past. For his cousin, Roman, it is the vision that together they can find fortune in Liberty City, gateway to the land of opportunity. As they slip into debt and are dragged into a criminal underworld by a series of shysters, thieves and sociopaths, they discover that the reality is very different from the dream in a city that worships money and status, and is heaven for those who have them an a living nightmare for those who don't.    \n",
       "\n",
       "   user_review game_of_the_year  \n",
       "0            1                0  \n",
       "1            1                0  \n",
       "2            1                1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.head(3) # View first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1c8b8aa-6881-481f-b8f5-49790f050481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "Name: meta_score, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.head(3) # first 3 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b455b9f-2ef2-4929-b03f-7c255f22f0b7",
   "metadata": {},
   "source": [
    "**Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39af6d7f-6de0-4f16-8f3b-01d9a3a833bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting test size to 20%, and training at 80%\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X2, y2, test_size=0.20, random_state=1) # same random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f5826ac-f4b6-4829-bf89-7dbf9712a461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train 2 shape: (14220, 4)\n",
      "X test 2 shape: (3556, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X train 2 shape: {X_train_2.shape}\")\n",
    "print(f\"X test 2 shape: {X_test_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51ce9380-0aa0-4f26-933f-279f59b83ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train 2 shape: (14220,)\n",
      "y test 2 shape: (3556,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y train 2 shape: {y_train_2.shape}\")\n",
    "print(f\"y test 2 shape: {y_test_2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3987758-264e-44dd-80ef-6c616ff2088b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c766323-748f-46f1-820a-b950cfe53989",
   "metadata": {},
   "source": [
    "**GridSearch 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bf8ae93-7431-4bac-af93-8faf39901ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "\n",
      "The best model used the following settings: \n",
      " Pipeline(steps=[('transformer',\n",
      "                 ColumnTransformer(transformers=[('platform_transform',\n",
      "                                                  OneHotEncoder(),\n",
      "                                                  ['platform']),\n",
      "                                                 ('summary_transform',\n",
      "                                                  CountVectorizer(min_df=5,\n",
      "                                                                  tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
      "                                                  'summary')])),\n",
      "                ('model',\n",
      "                 LogisticRegression(C=10, max_iter=10000, random_state=1))])\n",
      "Best Model Train Score (%): 96.58\n",
      "Best Model Test Score (%): 69.52\n"
     ]
    }
   ],
   "source": [
    "if models_loaded_flag: \n",
    "    print('Loading pre-trained model...')\n",
    "    gs3 = model_dict['gs3'] # reference the pkl\n",
    "    \n",
    "    # Print the best estimator\n",
    "    print('\\nThe best model used the following settings:', '\\n',gs3.best_estimator_)\n",
    "    \n",
    "    # Print score \n",
    "    print(f\"Best Model Train Score (%): {round(gs3.score(X_train_2, y_train_2)*100.00, 2)}\")\n",
    "    print(f\"Best Model Test Score (%): {round(gs3.score(X_test_2, y_test_2)*100.00,2)}\")\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "else:\n",
    "    \n",
    "    # State Models will re-train\n",
    "    print(\"Training Models...\")\n",
    "    \n",
    "    \n",
    "    # GRIDSEARCH 3:\n",
    "\n",
    "    estimators3 = [('transformer', ColumnTransformer([])),\n",
    "                ('model', LogisticRegression())]\n",
    "\n",
    "\n",
    "    pipe3 = Pipeline(estimators3)\n",
    "\n",
    "    # Instantiate Pipeline with the specified steps\n",
    "    param_grid3 = [\n",
    "\n",
    "    # Logistic Classifier\n",
    "                {'transformer': [ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']), #column transformer/tfidfvectorizer\n",
    "                    ('summary_transform', TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')]),\n",
    "                 ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']), #column transformer/countvectorizer\n",
    "                    ('summary_transform', CountVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')])],\n",
    "                'model': [LogisticRegression()],\n",
    "                'model__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "                'model__penalty': ['l2', 'none'],\n",
    "                'model__max_iter': [10000],\n",
    "                'model__random_state': [1]},\n",
    "\n",
    "    # # # Decision Tree Classifier\n",
    "                {'transformer': [ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']),\n",
    "                    ('summary_transform', TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')]),\n",
    "                 ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']),\n",
    "                    ('summary_transform', CountVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')])],\n",
    "                'model': [DecisionTreeClassifier()],\n",
    "                'model__max_depth': range(2, 6), #limits number of splits made/complexity\n",
    "                'model__min_samples_leaf':[2], #places lower bound on number of datapoints each region covers\n",
    "                'model__random_state': [1]},\n",
    "\n",
    "    # # # Random Forest\n",
    "               {'transformer': [ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']),\n",
    "                    ('summary_transform', TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')]),\n",
    "                 ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']),\n",
    "                    ('summary_transform', CountVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')])],\n",
    "                 'model': [RandomForestClassifier()],\n",
    "                 'model__max_depth': range(2, 6),\n",
    "                 'model__min_samples_leaf':[2],\n",
    "                 'model__random_state': [1]},\n",
    "                     ]\n",
    "\n",
    "    # Instate cross-validated grid search object\n",
    "    grid_3 = GridSearchCV(\n",
    "        estimator = pipe3,\n",
    "        param_grid = param_grid3,\n",
    "        cv = 5, # cross validation folds\n",
    "        verbose = 0,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    # fit the grid \n",
    "    fitted_grid3 = grid_3.fit(X_train_2, y_train_2)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------# \n",
    "\n",
    "# Saving GridSearch 3 as a pickle file\n",
    "   \n",
    "    #gs3 = fitted_grid3\n",
    "    joblib.dump(fitted_grid3, 'data/fitted_gridsearchcv3.pkl')\n",
    "    # Setting variable\n",
    "    gs3 = joblib.load('data/fitted_gridsearchcv3.pkl' ) # NOTE: Unusual, but doing the loading first before calling best estimator/scores since run times are quicker for me\n",
    "    \n",
    "    \n",
    "    # Best Estimator of GridSearch\n",
    "    print(gs3.best_estimator_)\n",
    "    \n",
    "    # Print Score\n",
    "    print(f\"Best Model Train Score (%): {round(gs3.score(X_train_2, y_train_2)*100.00, 2)}\")\n",
    "    print(f\"Best Model Test Score (%): {round(gs3.score(X_test_2, y_test_2)*100.00,2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3e0fe-60c5-4666-b2a2-5f3c22fbb57a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df58aae-38a6-4c4c-9d96-30762a5fcf56",
   "metadata": {},
   "source": [
    "#### GridSearchCV 3 Results <a id=\"d9.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2940b39d-3148-43eb-9365-5e712676bb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch 3 Cross Validation Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.675035</td>\n",
       "      <td>0.008078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.674473</td>\n",
       "      <td>0.008927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.673699</td>\n",
       "      <td>0.012215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.671941</td>\n",
       "      <td>0.009624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.01, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.671941</td>\n",
       "      <td>0.009624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.671941</td>\n",
       "      <td>0.009624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.671941</td>\n",
       "      <td>0.009624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.001, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.671941</td>\n",
       "      <td>0.009624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.001, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.669409</td>\n",
       "      <td>0.010187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.669409</td>\n",
       "      <td>0.010187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_score  \\\n",
       "17                1   \n",
       "13                2   \n",
       "16                3   \n",
       "18                4   \n",
       "6                 4   \n",
       "14                4   \n",
       "10                4   \n",
       "2                 4   \n",
       "3                 9   \n",
       "19                9   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                params  \\\n",
       "17       {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "13        {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "16       {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "18     {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "6    {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.01, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "14      {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "10    {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "2   {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.001, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "3   {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.001, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "19     {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "\n",
       "    mean_test_score  std_test_score  \n",
       "17         0.675035        0.008078  \n",
       "13         0.674473        0.008927  \n",
       "16         0.673699        0.012215  \n",
       "18         0.671941        0.009624  \n",
       "6          0.671941        0.009624  \n",
       "14         0.671941        0.009624  \n",
       "10         0.671941        0.009624  \n",
       "2          0.671941        0.009624  \n",
       "3          0.669409        0.010187  \n",
       "19         0.669409        0.010187  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs3_results_df = pd.DataFrame(gs3.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "gs3_results = gs3_results_df[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False) # Order by highest mean_test_score/5 fold cross validation\n",
    "print(\"GridSearch 3 Cross Validation Results\")\n",
    "gs3_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa40814-8998-4d1a-bda1-a70d8e63f32c",
   "metadata": {},
   "source": [
    "#### GridSearchCV 3 Summary <a id=\"d9.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f6ef6-ea39-4b3e-b8fc-32a65a0a25b2",
   "metadata": {},
   "source": [
    "- Interesting! The addition of the new features made it to the best model which is a Logistic Classifier, Count-vectorized, with a C=10. The test score improved nearly a percent more from ~68.8% to ~69.5%. This is some small good news. The std test score of the best model also seems to be quite low, which signifies less variance with the cross validation folds.\n",
    "\n",
    "- Now that we established that the addition of new features has helped the model performance and accuracy, I will run my last GridSearch with more parameter ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09748840-24c6-4941-913e-f490b0558ad9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff4ec09-4f63-4865-ac2a-f81f053972c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GridSearch 4 - More Parameter Range <a id=\"d10\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e07d01-9a48-466c-b8fd-2d791120c770",
   "metadata": {},
   "source": [
    "The GridSearch 4 will be an extension of GridSearch 3, with only a wider range of parameters. The new features were determined to help improve accuracy score and model performance, so these will be kept. The reason I put a wide range of parameters in this GridSearch is because I want to maximize the chance of getting the best model.\n",
    "\n",
    "**Fitted Models**:\n",
    "- Logistic\n",
    "- DecisionTree\n",
    "- RandomForest Classifiers\n",
    "    \n",
    "**Vectorizers**:\n",
    "- TF-IDF, Count Vectorizer\n",
    "\n",
    "**Features**:\n",
    "- Added features include: `game of the year`, `platform`, and `user review` via ColumnTransformer()\n",
    "\n",
    "\n",
    "**Run-Time**: 30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8937415-a4e1-4a1e-adec-c7b969c75e2b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab2acf-d9d9-4901-91c2-8c55c1d6bbb0",
   "metadata": {},
   "source": [
    "**GridSearch 4**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e0e3e6f-3449-460c-99f3-e48bce93fb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "\n",
      "The best model used the following settings: \n",
      " Pipeline(steps=[('transformer',\n",
      "                 ColumnTransformer(transformers=[('platform_transform',\n",
      "                                                  OneHotEncoder(),\n",
      "                                                  ['platform']),\n",
      "                                                 ('summary_transform',\n",
      "                                                  CountVectorizer(min_df=5,\n",
      "                                                                  tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
      "                                                  'summary')])),\n",
      "                ('model',\n",
      "                 LogisticRegression(C=10, max_iter=10000, random_state=1))])\n",
      "Best Model Train Score (%): 96.58\n",
      "Best Model Test Score (%): 69.52\n"
     ]
    }
   ],
   "source": [
    "if models_loaded_flag: \n",
    "    print('Loading pre-trained model...')\n",
    "    gs4 = model_dict['gs4'] # reference the pkl\n",
    "    \n",
    "    # Print the best estimator\n",
    "    print('\\nThe best model used the following settings:', '\\n',gs4.best_estimator_)\n",
    "    \n",
    "    # Print score \n",
    "    print(f\"Best Model Train Score (%): {round(gs4.score(X_train_2, y_train_2)*100.00, 2)}\")\n",
    "    print(f\"Best Model Test Score (%): {round(gs4.score(X_test_2, y_test_2)*100.00,2)}\")\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "else:\n",
    "    # State Models will re-train\n",
    "    print(\"Training Models...\")\n",
    "    \n",
    "    # GRIDSEARCH 4:\n",
    "\n",
    "    estimators4 = [('transformer', ColumnTransformer([])),\n",
    "                ('model', LogisticRegression())]\n",
    "\n",
    "\n",
    "    pipe4 = Pipeline(estimators4)\n",
    "\n",
    "    # Instantiate Pipeline with the specified steps\n",
    "    param_grid4 = [\n",
    "\n",
    "    # Logistic Classifier with new Features\n",
    "                {'transformer': [ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']),\n",
    "                    ('summary_transform', TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')]),\n",
    "                 ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']),\n",
    "                    ('summary_transform', CountVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')])],\n",
    "                'model': [LogisticRegression()],\n",
    "                'model__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], # More C values\n",
    "                'model__penalty': ['l2', 'none'],\n",
    "                'model__max_iter': [10000],\n",
    "                'model__random_state': [1]},\n",
    "\n",
    "    # # # Decision Tree Classifier\n",
    "                {'transformer': [ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']),\n",
    "                    ('summary_transform', TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')]),\n",
    "                 ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']),\n",
    "                    ('summary_transform', CountVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')])],\n",
    "                'model': [DecisionTreeClassifier()],\n",
    "                'model__max_depth': range(2, 10), #limits number of splits made/complexity\n",
    "                'model__min_samples_leaf':range(2, 6), #keeping this low to keep interpretability\n",
    "                'model__random_state': [1]},\n",
    "\n",
    "    # # # Random Forest\n",
    "               {'transformer': [ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']),\n",
    "                    ('summary_transform', TfidfVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')]),\n",
    "                 ColumnTransformer([('platform_transform', OneHotEncoder(), ['platform']),\n",
    "                    ('summary_transform', CountVectorizer(min_df=5, tokenizer=my_tokenizer), 'summary')])],\n",
    "                 'model': [RandomForestClassifier()],\n",
    "                 'model__max_depth': range(2, 10),\n",
    "                 'model__min_samples_leaf': range(2, 6), #keeping this low to keep interpretability \n",
    "                 'model__random_state': [1]},\n",
    "                     ]\n",
    "\n",
    "    # Instate cross-validated grid search object\n",
    "    grid_4 = GridSearchCV(\n",
    "        estimator = pipe4,\n",
    "        param_grid = param_grid4,\n",
    "        cv = 5, # cross validation folds\n",
    "        verbose = 0,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    # fit the grid \n",
    "    fitted_grid4 = grid_4.fit(X_train_2, y_train_2)\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------# \n",
    "\n",
    "# Saving GridSearch 4 as a pickle file\n",
    "   \n",
    "    joblib.dump(fitted_grid4, 'data/fitted_gridsearchcv4.pkl')\n",
    "    # Setting variable\n",
    "    gs4 = joblib.load('data/fitted_gridsearchcv4.pkl' ) # NOTE: Unusual, but doing the loading first before calling best estimator/scores since run times are quicker for me\n",
    "    \n",
    "    \n",
    "    # Best Estimator of GridSearch\n",
    "    print(gs4.best_estimator_)\n",
    "    \n",
    "    # Print Score\n",
    "    print(f\"Best Model Train Score (%): {round(gs4.score(X_train_2, y_train_2)*100.00, 2)}\")\n",
    "    print(f\"Best Model Test Score (%): {round(gs4.score(X_test_2, y_test_2)*100.00,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c92e6-7ca2-4200-ad2f-097b2e956cbc",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a34dc-a44c-4a7d-b2af-b75597fa9603",
   "metadata": {},
   "source": [
    "#### GridSearch 4 Results <a id=\"d10.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24bffab4-85e5-491b-a889-a18b735139dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch 4 Cross Validation Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.675035</td>\n",
       "      <td>0.008078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.674473</td>\n",
       "      <td>0.008927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.673699</td>\n",
       "      <td>0.012215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 100, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.672855</td>\n",
       "      <td>0.008109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.001, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.670816</td>\n",
       "      <td>0.009506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.670816</td>\n",
       "      <td>0.009506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 100, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.670816</td>\n",
       "      <td>0.009506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.670816</td>\n",
       "      <td>0.009506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 1000, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.670816</td>\n",
       "      <td>0.009506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>{'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\n",
       "                                 'summary')])}</td>\n",
       "      <td>0.670816</td>\n",
       "      <td>0.009506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_score  \\\n",
       "17                1   \n",
       "13                2   \n",
       "16                3   \n",
       "20                4   \n",
       "2                 5   \n",
       "10                5   \n",
       "22                5   \n",
       "18                5   \n",
       "26                5   \n",
       "14                5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                params  \\\n",
       "17       {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "13        {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 CountVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "16       {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "20      {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 100, 'model__max_iter': 10000, 'model__penalty': 'l2', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "2   {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.001, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "10    {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 0.1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "22    {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 100, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "18     {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 10, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "26   {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 1000, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "14      {'model': LogisticRegression(C=10, max_iter=10000, random_state=1), 'model__C': 1, 'model__max_iter': 10000, 'model__penalty': 'none', 'model__random_state': 1, 'transformer': ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\n",
       "                                 ['platform']),\n",
       "                                ('summary_transform',\n",
       "                                 TfidfVectorizer(min_df=5,\n",
       "                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\n",
       "                                 'summary')])}   \n",
       "\n",
       "    mean_test_score  std_test_score  \n",
       "17         0.675035        0.008078  \n",
       "13         0.674473        0.008927  \n",
       "16         0.673699        0.012215  \n",
       "20         0.672855        0.008109  \n",
       "2          0.670816        0.009506  \n",
       "10         0.670816        0.009506  \n",
       "22         0.670816        0.009506  \n",
       "18         0.670816        0.009506  \n",
       "26         0.670816        0.009506  \n",
       "14         0.670816        0.009506  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs4_results_df = pd.DataFrame(gs4.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "gs4_results = gs4_results_df[['rank_test_score', 'params', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False) # Order by highest mean_test_score/5 fold cross validation\n",
    "print(\"GridSearch 4 Cross Validation Results\")\n",
    "gs4_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac1bed8-9a75-4549-aff5-52ff6cda54e1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba75e9e0-b0e1-4fa9-8c10-6e4e28883685",
   "metadata": {},
   "source": [
    "#### GridSearch 4 Summary <a id=\"d10.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ce680-963a-4444-bfca-4f8924c6e786",
   "metadata": {},
   "source": [
    "- The results show the same model in GridSearch 3 made it as the best model in this final GridSearch (Logistic with a C=10). This model achieved a test score of 96.58%, which means the model trained incredibly well. \n",
    "- The model then tested with an accuracy score of **69.52%**. This was an improvement from the last model beforehand (68%.81).\n",
    "- This model is overfitting which leads me to suggest that the test data was having a difficult time generalizing to new data, even with regularization and cross validation applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a10d7-14bf-47fd-9c9c-efcb5315a1a2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef896c-8816-4f04-8774-b6c760896ef2",
   "metadata": {},
   "source": [
    "# Modeling Summary <a id=\"d11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3468e5d2-e2e7-42c2-b194-9a663a464911",
   "metadata": {},
   "source": [
    "**Compiling Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aeee7f48-dd59-4019-a7a4-bfb1b7a43c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List\n",
    "gs_full_results = [['Dummy','Dummy Classifier' ,'',round(accuracy_score(y_test, y_pred_baseline_t)*100, 2)],\n",
    "                   ['GRIDSEARCH 1', gs1.best_estimator_, round(gs1.score(X_train, y_train)*100.00, 2), round(gs1.score(X_test, y_test)*100.00,2)],\n",
    "                   ['GRIDSEARCH 2', gs2.best_estimator_, round(gs2.score(X_train, y_train)*100.00, 2), round(gs2.score(X_test, y_test)*100.00,2)],\n",
    "                   ['GRIDSEARCH 3', gs3.best_estimator_, round(gs3.score(X_train_2, y_train_2)*100.00, 2), round(gs3.score(X_test_2, y_test_2)*100.00,2)], \n",
    "                   ['GRIDSEARCH 4', gs4.best_estimator_, round(gs4.score(X_train_2, y_train_2)*100.00, 2), round(gs4.score(X_test_2, y_test_2)*100.00,2)]]\n",
    "\n",
    "# Create Dataframe\n",
    "gs_full_df = pd.DataFrame(gs_full_results, columns = ['Title', 'Best_Estimator', 'Best_Train_Score', 'Best_Test_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d790924b-86da-45b4-9fd4-e08eafcb5cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRIDSEARCH RESULTS SUMMARY\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Best_Estimator</th>\n",
       "      <th>Best_Train_Score</th>\n",
       "      <th>Best_Test_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dummy</td>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td></td>\n",
       "      <td>51.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRIDSEARCH 1</td>\n",
       "      <td>(TfidfVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;), KNeighborsClassifier(n_neighbors=3))</td>\n",
       "      <td>95.69</td>\n",
       "      <td>71.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRIDSEARCH 2</td>\n",
       "      <td>(CountVectorizer(min_df=5, tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;), LogisticRegression(C=1, max_iter=10000, random_state=1))</td>\n",
       "      <td>91.65</td>\n",
       "      <td>68.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRIDSEARCH 3</td>\n",
       "      <td>(ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\\n                                 ['platform']),\\n                                ('summary_transform',\\n                                 CountVectorizer(min_df=5,\\n                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\\n                                 'summary')]), LogisticRegression(C=10, max_iter=10000, random_state=1))</td>\n",
       "      <td>96.58</td>\n",
       "      <td>69.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRIDSEARCH 4</td>\n",
       "      <td>(ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\\n                                 ['platform']),\\n                                ('summary_transform',\\n                                 CountVectorizer(min_df=5,\\n                                                 tokenizer=&lt;function my_tokenizer at 0x7fca129c9550&gt;),\\n                                 'summary')]), LogisticRegression(C=10, max_iter=10000, random_state=1))</td>\n",
       "      <td>96.58</td>\n",
       "      <td>69.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Title  \\\n",
       "0         Dummy   \n",
       "1  GRIDSEARCH 1   \n",
       "2  GRIDSEARCH 2   \n",
       "3  GRIDSEARCH 3   \n",
       "4  GRIDSEARCH 4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                   Best_Estimator  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                Dummy Classifier   \n",
       "1                                                                                                                                                                                                                                                                                                                                           (TfidfVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>), KNeighborsClassifier(n_neighbors=3))   \n",
       "2                                                                                                                                                                                                                                                                                                                       (CountVectorizer(min_df=5, tokenizer=<function my_tokenizer at 0x7fca129c9550>), LogisticRegression(C=1, max_iter=10000, random_state=1))   \n",
       "3  (ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\\n                                 ['platform']),\\n                                ('summary_transform',\\n                                 CountVectorizer(min_df=5,\\n                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\\n                                 'summary')]), LogisticRegression(C=10, max_iter=10000, random_state=1))   \n",
       "4  (ColumnTransformer(transformers=[('platform_transform', OneHotEncoder(),\\n                                 ['platform']),\\n                                ('summary_transform',\\n                                 CountVectorizer(min_df=5,\\n                                                 tokenizer=<function my_tokenizer at 0x7fca129c9550>),\\n                                 'summary')]), LogisticRegression(C=10, max_iter=10000, random_state=1))   \n",
       "\n",
       "  Best_Train_Score  Best_Test_Score  \n",
       "0                             51.07  \n",
       "1            95.69            71.23  \n",
       "2            91.65            68.81  \n",
       "3            96.58            69.52  \n",
       "4            96.58            69.52  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment the next row to expand the best_estimator row cells\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"GRIDSEARCH RESULTS SUMMARY\")\n",
    "gs_full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47dd8b7-4a29-4488-b752-bbfd46bd140d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5699174-f589-427b-97b3-75dc1ae142ce",
   "metadata": {},
   "source": [
    "**Save Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ee761-967b-4eb8-b14a-7309afe18a09",
   "metadata": {},
   "source": [
    "[Saving DF to Pickle](https://www.codegrepper.com/code-examples/python/python+save+dataframe+to+pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "badb441a-5178-4476-b914-14964b105b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_full_df.to_pickle('data/gs_full_df.pkl') # save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def75d48-2091-44d6-a55e-36c0e65ba4a5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f008232-4523-4b63-ab39-b9f317fe5e52",
   "metadata": {},
   "source": [
    "**Notebook Ending Remarks**\n",
    "\n",
    "The next notebook will go over the findings, insights, and the results in more detail. See Notebook *5-Findings and Results*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f764d-1893-44a4-9489-c5270a0675a3",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
